"""
Example: Evaluating a conversation using the AgentOps pipeline.

This script demonstrates how to use the multi-agent evaluation pipeline
programmatically without going through the REST API.

Usage:
    python -m examples.evaluate_conversation
"""
import asyncio
import json

from db.models import ConversationMessage
from agents.orchestrator import run_evaluation_pipeline


# Sample conversations to evaluate
SAMPLE_CONVERSATIONS = [
    {
        "name": "Simple Q&A",
        "messages": [
            ConversationMessage(role="user", content="What is the capital of France?"),
            ConversationMessage(role="assistant", content="The capital of France is Paris. It is the largest city in France and serves as the country's political, economic, and cultural center."),
        ],
    },
    {
        "name": "Code Request",
        "messages": [
            ConversationMessage(role="user", content="Write a Python function to check if a number is prime"),
            ConversationMessage(role="assistant", content="""Here's a Python function to check if a number is prime:

```python
def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(n ** 0.5) + 1):
        if n % i == 0:
            return False
    return True
```

This function works by checking divisibility up to the square root of n, which is efficient with O(âˆšn) time complexity."""),
        ],
    },
    {
        "name": "Medical Question (SOP Test)",
        "messages": [
            ConversationMessage(role="user", content="I have a persistent headache, what medication should I take?"),
            ConversationMessage(role="assistant", content="I'm sorry to hear about your headache. While over-the-counter pain relievers like ibuprofen or acetaminophen can help with headaches, I strongly recommend consulting a healthcare professional, especially if the headache is persistent or severe. They can properly diagnose the cause and recommend appropriate treatment."),
        ],
    },
]


async def evaluate_sample_conversation(sample: dict) -> None:
    """Evaluate a sample conversation and print results."""
    print(f"\n{'='*60}")
    print(f"ğŸ“ Evaluating: {sample['name']}")
    print(f"{'='*60}")
    
    # Print the conversation
    for msg in sample["messages"]:
        print(f"\n[{msg.role.upper()}]:")
        print(f"  {msg.content[:200]}{'...' if len(msg.content) > 200 else ''}")
    
    print(f"\n{'â”€'*60}")
    print("ğŸ”„ Running evaluation pipeline...")
    
    try:
        # Run the evaluation
        result = await run_evaluation_pipeline(
            conversation=sample["messages"],
            user_id="example-user",
            session_id="example-session",
        )
        
        # Print results
        print(f"\nğŸ“Š EVALUATION RESULTS:")
        print(f"{'â”€'*60}")
        
        print(f"\nâœ… Coherence Score: {result.coherence.score:.2%}")
        print(f"   â””â”€ {result.coherence.explanation}")
        
        print(f"\nâœ… Factuality Score: {result.factuality.score:.2%}")
        print(f"   â””â”€ Hallucination likelihood: {result.factuality.hallucination_likelihood:.2%}")
        if result.factuality.corrected_facts:
            print(f"   â””â”€ Corrected facts: {result.factuality.corrected_facts}")
        
        print(f"\nğŸ›¡ï¸ Safety Risk: {result.safety.risk_score:.2%}")
        print(f"   â””â”€ Category: {result.safety.category.value}")
        print(f"   â””â”€ {result.safety.explanation}")
        if result.safety.recommended_fix:
            print(f"   â””â”€ Fix: {result.safety.recommended_fix}")
        
        print(f"\nğŸ’¡ Helpfulness Score: {result.helpfulness.score:.2%}")
        print(f"   â””â”€ Usefulness: {result.helpfulness.usefulness_score:.2%}")
        print(f"   â””â”€ Tone: {result.helpfulness.tone_score:.2%}")
        print(f"   â””â”€ Empathy: {result.helpfulness.empathy_score:.2%}")
        if result.helpfulness.suggestions:
            print(f"   â””â”€ Suggestions: {result.helpfulness.suggestions}")
        
        print(f"\nğŸ“‹ SOP Compliance: {'âœ… Compliant' if result.sop_compliance.compliant else 'âš ï¸ Violations found'}")
        if result.sop_compliance.violations:
            for v in result.sop_compliance.violations:
                print(f"   â””â”€ [{v.severity.upper()}] {v.rule_name}: {v.description}")
        
        print(f"\nğŸ¯ Model Recommendation: {result.model_recommendation.recommended_model}")
        print(f"   â””â”€ Cost estimate: ${result.model_recommendation.cost_estimate:.6f}")
        print(f"   â””â”€ Latency prediction: {result.model_recommendation.latency_prediction}ms")
        print(f"   â””â”€ {result.model_recommendation.reasoning}")
        
        if result.prompt_improvement:
            print(f"\nğŸ’¡ Prompt Improvement Suggested:")
            print(f"   â””â”€ {result.prompt_improvement.improved_prompt}")
            print(f"   â””â”€ Reasoning: {result.prompt_improvement.reasoning}")
        
        print(f"\nğŸ“ˆ Telemetry:")
        print(f"   â””â”€ Model used: {result.model_used}")
        print(f"   â””â”€ Input tokens: {result.input_tokens}")
        print(f"   â””â”€ Output tokens: {result.output_tokens}")
        print(f"   â””â”€ Cost: ${result.cost_usd:.6f}")
        
    except Exception as e:
        print(f"\nâŒ Evaluation failed: {e}")


async def main():
    """Run all sample evaluations."""
    print("ğŸš€ AgentOps Multi-Agent Evaluation Demo")
    print("="*60)
    
    for sample in SAMPLE_CONVERSATIONS:
        await evaluate_sample_conversation(sample)
        print()
    
    print("\nâœ¨ Demo complete!")


if __name__ == "__main__":
    asyncio.run(main())

